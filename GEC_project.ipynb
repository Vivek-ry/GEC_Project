{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5f3dcbf",
   "metadata": {},
   "source": [
    "## Grammatical Error Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f26b044b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install -q datasets tqdm pandas\n",
    "# !pip install -q sentencepiece\n",
    "# !pip install -q transformers\n",
    "# !pip install -q wandb\n",
    "#!pip install --user -U nltk\n",
    "#!pip3 install datasets\n",
    "#!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6e2c5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import tensorflow_datasets\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#import datasets\n",
    "from transformers import AdamW,get_linear_schedule_with_warmup\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from datasets import load_metric\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84214466",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f92e897",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb1d6c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_df = 18386520  # C4_200M.tsv-00000-of-00010\n",
    "start =  1838651 # 919325 +\n",
    "end = int(18386520/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afeb26f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = end + 1\n",
    "end = start + 300000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "253c454d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "919327 1219327 300000\n"
     ]
    }
   ],
   "source": [
    "print(start, end, end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8133eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Korean translation agency based in London, The United Kingdom offers services in addition by official Korean translators.</th>\n",
       "      <th>Korean translation agency based in London, United Kingdom offering services by official Korean translators.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stab that badboy on with a stick!!</td>\n",
       "      <td>Stab that badboy with a stick!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What I did for the matrics to finish at this univercity is quite a clear example of what I described in this previous post, precisely the difficulties which the Italian citizens on a daily basis - when they have to deal with the public administration.</td>\n",
       "      <td>What I did to complete the matriculation at this University is a clear example of what I described in this previous post, namely the difficulties which the Italian citizens experience on a daily basis when they have to deal with the public administration.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The campaign was a big test for the newly appointed Regional Commissioner (RC) as president-Magufuli and prime minister Kassim Majaliwa insisting that school desks campaign was the RCs’ factor when appointed.</td>\n",
       "      <td>The campaign was a big test to the newly appointed Regional Commissioners (RC’s) as President Magufuli and Prime Minister Kassim Majaliwa were insisting that the school desks campaign was one of the RCs’ performance ratings after being appointed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Then he looed up canister, then found that it was a box for holding teas 33387 and when he turned to tea he discovered it was sometimes made of beef, and beef was meat and meat is what human being composed of; and canister was, therefore, a box for taking meat.</td>\n",
       "      <td>Then he looked up canister, and found that it was a box for holding tea; and when he turned to tea he discovered it was sometimes made of beef, and beef was meat, and meat is what human beings are composed of; and canister was, therefore, a box for containing meat.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Super-Quad CLASSIC+ MM - all the classic features of our prestigious Super-Quad pickups in their traditional solid carbon fibre housing, now with Music Man string spacing and poles piece.</td>\n",
       "      <td>Super-Quad CLASSIC+ MM - all the classic features of our renowned Super-Quad pickups in their traditional solid carbon fibre housing, now with Music Man string spacing and pole pieces.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                               Korean translation agency based in London, The United Kingdom offers services in addition by official Korean translators.  \\\n",
       "0                                                                                                                                                                                                                                     Stab that badboy on with a stick!!   \n",
       "1            What I did for the matrics to finish at this univercity is quite a clear example of what I described in this previous post, precisely the difficulties which the Italian citizens on a daily basis - when they have to deal with the public administration.   \n",
       "2                                                       The campaign was a big test for the newly appointed Regional Commissioner (RC) as president-Magufuli and prime minister Kassim Majaliwa insisting that school desks campaign was the RCs’ factor when appointed.   \n",
       "3  Then he looed up canister, then found that it was a box for holding teas 33387 and when he turned to tea he discovered it was sometimes made of beef, and beef was meat and meat is what human being composed of; and canister was, therefore, a box for taking meat.   \n",
       "4                                                                            Super-Quad CLASSIC+ MM - all the classic features of our prestigious Super-Quad pickups in their traditional solid carbon fibre housing, now with Music Man string spacing and poles piece.   \n",
       "\n",
       "                                                                                                                                                                 Korean translation agency based in London, United Kingdom offering services by official Korean translators.  \n",
       "0                                                                                                                                                                                                                                            Stab that badboy with a stick!!  \n",
       "1            What I did to complete the matriculation at this University is a clear example of what I described in this previous post, namely the difficulties which the Italian citizens experience on a daily basis when they have to deal with the public administration.  \n",
       "2                     The campaign was a big test to the newly appointed Regional Commissioners (RC’s) as President Magufuli and Prime Minister Kassim Majaliwa were insisting that the school desks campaign was one of the RCs’ performance ratings after being appointed.  \n",
       "3  Then he looked up canister, and found that it was a box for holding tea; and when he turned to tea he discovered it was sometimes made of beef, and beef was meat, and meat is what human beings are composed of; and canister was, therefore, a box for containing meat.  \n",
       "4                                                                                   Super-Quad CLASSIC+ MM - all the classic features of our renowned Super-Quad pickups in their traditional solid carbon fibre housing, now with Music Man string spacing and pole pieces.  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name='C4_200M.tsv-00000-of-00010'\n",
    "df_main = pd.read_csv(file_name, delimiter='\\t', skiprows=start, nrows=end) # on_bad_lines='skip\n",
    "df_main.dropna(inplace=True)\n",
    "df_main.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6e48e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.columns = [\"input\", \"target\"]\n",
    "df=df_main.iloc[start:end]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "169d5bd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5.py:164: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = 't5-base'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4378caf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_token_len(example):\n",
    "    return len(tokenizer(example).input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdf199ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((269997, 2), (30000, 2))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.10, shuffle=True)\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "266a3727",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (911 > 512). Running this sequence through the model will result in indexing errors\n",
      "/tmp/ipykernel_15226/2133573097.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['input_token_len'] = test_df['input'].apply(calc_token_len)\n"
     ]
    }
   ],
   "source": [
    "test_df['input_token_len'] = test_df['input'].apply(calc_token_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8db6870d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "      <th>input_token_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>924269</th>\n",
       "      <td>Add the parsley (you can use the stems too, as long as you thinly chop them) to the maison.</td>\n",
       "      <td>Add the parsley (you can use the stems too, as long as you thinly chop them) to the bowl too.</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971103</th>\n",
       "      <td>ICE‟s 2009 report noted that despite the rapid growth of immigration detention genrally, number of convicted criminals located and detained its people barely increased.</td>\n",
       "      <td>ICE‟s 2009 report noted that despite the rapid growth of immigration detention generally, the number of convicted criminals located and detained had barely increased.</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116905</th>\n",
       "      <td>This link with BUILA proved invaluable with the University hosting BUILA’s annual conference over two consecutive years where Bobby had been first ‘enrolled’ as a USW Conference Ambassador!</td>\n",
       "      <td>This link with BUILA proved invaluable with the University hosting BUILA’s annual conference over two consecutive years where Bobby was first ‘enrolled’ as a USW Conference Ambassador!</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1218648</th>\n",
       "      <td>We focused and committed to making individuals improved lives or personal performance in a specific areas like stress management, starting up a small business and helping it grow time management, health coaching, personal relationships or other areas.</td>\n",
       "      <td>We focused and committed to helping individuals improve their lives or personal performance in a specific areas like stress management, starting up a small business and helping it grow, time management, health coaching, personal relationships or other areas.</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003253</th>\n",
       "      <td>Watch this video - our client fan about her perfect appointment to prestigious position - we pair it up with her Press Release.</td>\n",
       "      <td>Watch this video - our client talking about her appointment to a prestigious position - we paired it with her Press Release.</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                               input  \\\n",
       "924269                                                                                                                                                                   Add the parsley (you can use the stems too, as long as you thinly chop them) to the maison.   \n",
       "971103                                                                                      ICE‟s 2009 report noted that despite the rapid growth of immigration detention genrally, number of convicted criminals located and detained its people barely increased.   \n",
       "1116905                                                                This link with BUILA proved invaluable with the University hosting BUILA’s annual conference over two consecutive years where Bobby had been first ‘enrolled’ as a USW Conference Ambassador!   \n",
       "1218648  We focused and committed to making individuals improved lives or personal performance in a specific areas like stress management, starting up a small business and helping it grow time management, health coaching, personal relationships or other areas.   \n",
       "1003253                                                                                                                              Watch this video - our client fan about her perfect appointment to prestigious position - we pair it up with her Press Release.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                     target  \\\n",
       "924269                                                                                                                                                                        Add the parsley (you can use the stems too, as long as you thinly chop them) to the bowl too.   \n",
       "971103                                                                                               ICE‟s 2009 report noted that despite the rapid growth of immigration detention generally, the number of convicted criminals located and detained had barely increased.   \n",
       "1116905                                                                            This link with BUILA proved invaluable with the University hosting BUILA’s annual conference over two consecutive years where Bobby was first ‘enrolled’ as a USW Conference Ambassador!   \n",
       "1218648  We focused and committed to helping individuals improve their lives or personal performance in a specific areas like stress management, starting up a small business and helping it grow, time management, health coaching, personal relationships or other areas.   \n",
       "1003253                                                                                                                                        Watch this video - our client talking about her appointment to a prestigious position - we paired it with her Press Release.   \n",
       "\n",
       "         input_token_len  \n",
       "924269                27  \n",
       "971103                38  \n",
       "1116905               40  \n",
       "1218648               44  \n",
       "1003253               28  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7261792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    30000.000000\n",
       "mean        33.849933\n",
       "std         26.730257\n",
       "min          3.000000\n",
       "25%         17.000000\n",
       "50%         27.000000\n",
       "75%         42.000000\n",
       "max        917.000000\n",
       "Name: input_token_len, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['input_token_len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a904a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86e31362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'target', 'input_token_len', '__index_level_0__'],\n",
       "    num_rows: 30000\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7c7cb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrammarDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer,print_text=False):\n",
    "        self.dataset = dataset\n",
    "        self.pad_to_max_length = False\n",
    "        self.tokenizer = tokenizer\n",
    "        self.print_text = print_text\n",
    "        self.max_len = 64\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "    def tokenize_data(self, example):\n",
    "        input_, target_ = example['input'], example['target'] # output\n",
    "\n",
    "        # tokenize inputs\n",
    "        tokenized_inputs = tokenizer(input_, pad_to_max_length=self.pad_to_max_length,\n",
    "                                            max_length=self.max_len,\n",
    "                                            return_attention_mask=True)\n",
    "\n",
    "        tokenized_targets = tokenizer(target_, pad_to_max_length=self.pad_to_max_length,\n",
    "                                            max_length=self.max_len,\n",
    "                                            return_attention_mask=True)\n",
    "\n",
    "        inputs={\"input_ids\": tokenized_inputs['input_ids'],\n",
    "            \"attention_mask\": tokenized_inputs['attention_mask'],\n",
    "            \"labels\": tokenized_targets['input_ids']\n",
    "        }\n",
    "\n",
    "        return inputs\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        inputs = self.tokenize_data(self.dataset[index])\n",
    "\n",
    "        if self.print_text:\n",
    "            for k in inputs.keys():\n",
    "                print(k, len(inputs[k]))\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa1a77df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids 40\n",
      "attention_mask 40\n",
      "labels 38\n",
      "{'input_ids': [11107, 6, 16, 131, 7643, 767, 16, 828, 6, 13346, 12524, 5073, 23, 141, 131, 263, 2314, 8305, 21, 31786, 6, 17029, 11, 3411, 2348, 57, 8, 907, 1323, 6, 27274, 6, 52, 2051, 6, 11, 377, 17279, 5, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [11107, 6, 16, 131, 7643, 767, 16, 828, 6, 13346, 12524, 5073, 23, 65, 263, 2314, 8305, 12, 31786, 6, 17029, 11, 3411, 2348, 57, 8, 907, 1323, 6, 27274, 6, 2051, 6, 11, 377, 17279, 5, 1]}\n"
     ]
    }
   ],
   "source": [
    "dataset = GrammarDataset(test_dataset, tokenizer, True)\n",
    "print(dataset[121])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b895a72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15226/2048908469.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge_metric = load_metric(\"rouge\")\n"
     ]
    }
   ],
   "source": [
    "rouge_metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11eddb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding='longest', return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6474c601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "args = Seq2SeqTrainingArguments(\n",
    "                        output_dir=\"./kaggle/working/c4_200m/weights\",\n",
    "                        evaluation_strategy=\"steps\",\n",
    "                        per_device_train_batch_size=batch_size,\n",
    "                        per_device_eval_batch_size=batch_size,\n",
    "                        learning_rate=2e-5,\n",
    "                        num_train_epochs=1,\n",
    "                        weight_decay=0.01,\n",
    "                        save_total_limit=2,\n",
    "                        predict_with_generate=True,\n",
    "                        #fp16 = True, # only while using CUDA\n",
    "                        gradient_accumulation_steps = 6,\n",
    "                        eval_steps = 500,\n",
    "                        save_steps = 500,\n",
    "                        load_best_model_at_end=True,\n",
    "                        logging_dir=\"./logs\",\n",
    "                        report_to=None\n",
    "                        #report_to=\"wandb\", # report  to wandb\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "823775bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "\n",
    "    result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract a few results\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "\n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fef9e987",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(model=model,\n",
    "                args=args,\n",
    "                train_dataset= GrammarDataset(train_dataset, tokenizer),\n",
    "                eval_dataset=GrammarDataset(test_dataset, tokenizer),\n",
    "                tokenizer=tokenizer,\n",
    "                data_collator=data_collator,\n",
    "                compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "64798c30",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 269997\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 96\n",
      "  Gradient Accumulation steps = 6\n",
      "  Total optimization steps = 2812\n",
      "  Number of trainable parameters = 222903552\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2812' max='2812' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2812/2812 2:47:59, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.763400</td>\n",
       "      <td>0.628792</td>\n",
       "      <td>71.224700</td>\n",
       "      <td>60.843600</td>\n",
       "      <td>70.482700</td>\n",
       "      <td>70.513800</td>\n",
       "      <td>17.331700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.678100</td>\n",
       "      <td>0.603869</td>\n",
       "      <td>71.444600</td>\n",
       "      <td>61.254500</td>\n",
       "      <td>70.707600</td>\n",
       "      <td>70.743500</td>\n",
       "      <td>17.316600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.656000</td>\n",
       "      <td>0.591053</td>\n",
       "      <td>71.598400</td>\n",
       "      <td>61.518200</td>\n",
       "      <td>70.867500</td>\n",
       "      <td>70.903300</td>\n",
       "      <td>17.300400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.644400</td>\n",
       "      <td>0.585335</td>\n",
       "      <td>71.653500</td>\n",
       "      <td>61.625700</td>\n",
       "      <td>70.919300</td>\n",
       "      <td>70.954400</td>\n",
       "      <td>17.299000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.637800</td>\n",
       "      <td>0.582086</td>\n",
       "      <td>71.696400</td>\n",
       "      <td>61.697600</td>\n",
       "      <td>70.964500</td>\n",
       "      <td>71.000800</td>\n",
       "      <td>17.296600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./kaggle/working/c4_200m/weights/checkpoint-500\n",
      "Configuration saved in ./kaggle/working/c4_200m/weights/checkpoint-500/config.json\n",
      "Model weights saved in ./kaggle/working/c4_200m/weights/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./kaggle/working/c4_200m/weights/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./kaggle/working/c4_200m/weights/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [kaggle/working/c4_200m/weights/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./kaggle/working/c4_200m/weights/checkpoint-1000\n",
      "Configuration saved in ./kaggle/working/c4_200m/weights/checkpoint-1000/config.json\n",
      "Model weights saved in ./kaggle/working/c4_200m/weights/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./kaggle/working/c4_200m/weights/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./kaggle/working/c4_200m/weights/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [kaggle/working/c4_200m/weights/checkpoint-1500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./kaggle/working/c4_200m/weights/checkpoint-1500\n",
      "Configuration saved in ./kaggle/working/c4_200m/weights/checkpoint-1500/config.json\n",
      "Model weights saved in ./kaggle/working/c4_200m/weights/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./kaggle/working/c4_200m/weights/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./kaggle/working/c4_200m/weights/checkpoint-1500/special_tokens_map.json\n",
      "Deleting older checkpoint [kaggle/working/c4_200m/weights/checkpoint-500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./kaggle/working/c4_200m/weights/checkpoint-2000\n",
      "Configuration saved in ./kaggle/working/c4_200m/weights/checkpoint-2000/config.json\n",
      "Model weights saved in ./kaggle/working/c4_200m/weights/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./kaggle/working/c4_200m/weights/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./kaggle/working/c4_200m/weights/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [kaggle/working/c4_200m/weights/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./kaggle/working/c4_200m/weights/checkpoint-2500\n",
      "Configuration saved in ./kaggle/working/c4_200m/weights/checkpoint-2500/config.json\n",
      "Model weights saved in ./kaggle/working/c4_200m/weights/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./kaggle/working/c4_200m/weights/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./kaggle/working/c4_200m/weights/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [kaggle/working/c4_200m/weights/checkpoint-1500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./kaggle/working/c4_200m/weights/checkpoint-2500 (score: 0.5820855498313904).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2812, training_loss=0.6711784297677226, metrics={'train_runtime': 10081.2185, 'train_samples_per_second': 26.782, 'train_steps_per_second': 0.279, 'total_flos': 1.986856424497152e+16, 'train_loss': 0.6711784297677226, 'epoch': 1.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "02208022",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to t5_gec_model_03\n",
      "Configuration saved in t5_gec_model_03/config.json\n",
      "Model weights saved in t5_gec_model_03/pytorch_model.bin\n",
      "tokenizer config file saved in t5_gec_model_03/tokenizer_config.json\n",
      "Special tokens file saved in t5_gec_model_03/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "model_name='t5_gec_model_03'\n",
    "trainer.save_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "00b5cdbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file spiece.model\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file t5_gec_model_03/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file t5_gec_model_03/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5_gec_model_03.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_name = 't5_gec_model_03'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "\n",
    "def correct_grammar(input_text,num_return_sequences):\n",
    "    batch = tokenizer([input_text],truncation=True,padding='max_length',max_length=64, return_tensors=\"pt\").to(torch_device)\n",
    "    translated = model.generate(**batch,max_length=64,num_beams=4, num_return_sequences=num_return_sequences, temperature=1.5)\n",
    "    tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    return tgt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "40288f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I enjoy writing articles on AI and I also enjoy writing articles on AI.']\n"
     ]
    }
   ],
   "source": [
    "input_text = \"I am enjoys, writtings Articles ons AI and I also enjoyed write articling on AI.\"\n",
    "num_return_sequences = 1\n",
    "corrected_text = correct_grammar(input_text, num_return_sequences)\n",
    "print(corrected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "230ff615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Today gift shows are popular in many countries, and the purpose of these shows is to find talented people, and help them to introduce themselves to each other.Actually, many people now watch these shows, and during this years find more fans that increase the Viewer, and many sponsors.']\n"
     ]
    }
   ],
   "source": [
    "text =  \"\"\"Today gift shows are popular in many countries, and purpose of these shows finds talented people, and help them to introduce themselves to each other .Actually, many people now watch this shows, and during this years find more fans that cause increase the Viewer, and many sponsors Keen on for sponsoring this shows, because gift shows has benefits for them, and this programs convert to tools that earn money, and present their services. \n",
    "\n",
    "Firstly, result this programme has  a massive effect on the society, because many people get a chance to represent their gift. On the other hand, many people have gift, but they do not know, so they have the opportunity to find their gift, and encourage them to follow their interests.\n",
    "\n",
    "secondly, many audiences, and viewers watch this shows, so it is a big chance for companies by sponsoring in this program. They can find new customers and introduce their services to each other.For instance, they commercials between the shows certify this issue.Furthermore TV is one of the tools that entertain people, although the target finds gift, so part of this shows for entertaining people.\n",
    "\n",
    "As a result, the aim of  producing this shows impressive, so part of the society following this shows for entertaining, and the part of the people persuade to find their talents. In fact, this topic has two side that everyone can according to own opinion.\n",
    "\n",
    "\"\"\"\n",
    "print(correct_grammar(text, num_return_sequences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55edc780",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
